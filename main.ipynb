{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-18 12:38:48--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1,1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1,06M  6,55MB/s    in 0,2s    \n",
      "\n",
      "2024-07-18 12:38:49 (6,55 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of text: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size:  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) then target is 47\n",
      "when input is tensor([18, 47]) then target is 56\n",
      "when input is tensor([18, 47, 56]) then target is 57\n",
      "when input is tensor([18, 47, 56, 57]) then target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) then target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) then target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) then target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} then target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8]) torch.int64\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:  torch.Size([4, 8]) torch.int64\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] then target is 43\n",
      "when input is [24, 43] then target is 58\n",
      "when input is [24, 43, 58] then target is 5\n",
      "when input is [24, 43, 58, 5] then target is 57\n",
      "when input is [24, 43, 58, 5, 57] then target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] then target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] then target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] then target is 39\n",
      "when input is [44] then target is 53\n",
      "when input is [44, 53] then target is 56\n",
      "when input is [44, 53, 56] then target is 1\n",
      "when input is [44, 53, 56, 1] then target is 58\n",
      "when input is [44, 53, 56, 1, 58] then target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] then target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] then target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] then target is 1\n",
      "when input is [52] then target is 58\n",
      "when input is [52, 58] then target is 1\n",
      "when input is [52, 58, 1] then target is 58\n",
      "when input is [52, 58, 1, 58] then target is 46\n",
      "when input is [52, 58, 1, 58, 46] then target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] then target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] then target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] then target is 46\n",
      "when input is [25] then target is 17\n",
      "when input is [25, 17] then target is 27\n",
      "when input is [25, 17, 27] then target is 10\n",
      "when input is [25, 17, 27, 10] then target is 0\n",
      "when input is [25, 17, 27, 10, 0] then target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] then target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] then target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] then target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(data.size(0) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + 1 + block_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs: ', xb.shape, xb.dtype)\n",
    "print(xb)\n",
    "print('targets: ', yb.shape, yb.dtype)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} then target is {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "gen_seq = m.generate(idx, max_new_tokens=100)\n",
    "print(decode(gen_seq[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.616670846939087\n",
      "loss: 2.642460823059082\n",
      "loss: 2.4217329025268555\n",
      "loss: 2.6373305320739746\n",
      "loss: 2.5708348751068115\n",
      "loss: 2.5370733737945557\n",
      "loss: 2.6140685081481934\n",
      "loss: 2.539964199066162\n",
      "loss: 2.7259726524353027\n",
      "loss: 2.5714969635009766\n",
      "loss: 2.4518954753875732\n",
      "loss: 2.671290874481201\n",
      "loss: 2.590604305267334\n",
      "loss: 2.5630486011505127\n",
      "loss: 2.5906753540039062\n",
      "loss: 2.6879000663757324\n",
      "loss: 2.5954761505126953\n",
      "loss: 2.535547971725464\n",
      "loss: 2.5478150844573975\n",
      "loss: 2.520815372467041\n",
      "loss: 2.422400712966919\n",
      "loss: 2.49772572517395\n",
      "loss: 2.495818853378296\n",
      "loss: 2.459104299545288\n",
      "loss: 2.4867002964019775\n",
      "loss: 2.553175926208496\n",
      "loss: 2.530959129333496\n",
      "loss: 2.6377639770507812\n",
      "loss: 2.493283748626709\n",
      "loss: 2.500706672668457\n",
      "loss: 2.620847225189209\n",
      "loss: 2.5842771530151367\n",
      "loss: 2.5822267532348633\n",
      "loss: 2.5345070362091064\n",
      "loss: 2.507331132888794\n",
      "loss: 2.698512315750122\n",
      "loss: 2.414506673812866\n",
      "loss: 2.4676430225372314\n",
      "loss: 2.4631714820861816\n",
      "loss: 2.54647159576416\n",
      "loss: 2.5181472301483154\n",
      "loss: 2.6442224979400635\n",
      "loss: 2.4367926120758057\n",
      "loss: 2.5518341064453125\n",
      "loss: 2.52105975151062\n",
      "loss: 2.5188186168670654\n",
      "loss: 2.4679126739501953\n",
      "loss: 2.640284776687622\n",
      "loss: 2.426476001739502\n",
      "loss: 2.465937852859497\n",
      "loss: 2.5338611602783203\n",
      "loss: 2.648552417755127\n",
      "loss: 2.5931386947631836\n",
      "loss: 2.512059450149536\n",
      "loss: 2.476957082748413\n",
      "loss: 2.5152039527893066\n",
      "loss: 2.553595781326294\n",
      "loss: 2.5575129985809326\n",
      "loss: 2.5789175033569336\n",
      "loss: 2.635948896408081\n",
      "loss: 2.440380573272705\n",
      "loss: 2.6577231884002686\n",
      "loss: 2.4099953174591064\n",
      "loss: 2.425527572631836\n",
      "loss: 2.5164194107055664\n",
      "loss: 2.4295921325683594\n",
      "loss: 2.5614006519317627\n",
      "loss: 2.5966460704803467\n",
      "loss: 2.6435182094573975\n",
      "loss: 2.620936870574951\n",
      "loss: 2.5125505924224854\n",
      "loss: 2.5267951488494873\n",
      "loss: 2.500539541244507\n",
      "loss: 2.6147682666778564\n",
      "loss: 2.4928908348083496\n",
      "loss: 2.5804238319396973\n",
      "loss: 2.47162127494812\n",
      "loss: 2.513672113418579\n",
      "loss: 2.495934247970581\n",
      "loss: 2.5245120525360107\n",
      "loss: 2.4905877113342285\n",
      "loss: 2.427839756011963\n",
      "loss: 2.5621273517608643\n",
      "loss: 2.5694591999053955\n",
      "loss: 2.543060779571533\n",
      "loss: 2.4235165119171143\n",
      "loss: 2.5325522422790527\n",
      "loss: 2.6324515342712402\n",
      "loss: 2.6495325565338135\n",
      "loss: 2.5875847339630127\n",
      "loss: 2.537128448486328\n",
      "loss: 2.4458792209625244\n",
      "loss: 2.498927116394043\n",
      "loss: 2.519425868988037\n",
      "loss: 2.479854106903076\n",
      "loss: 2.5187435150146484\n",
      "loss: 2.483156442642212\n",
      "loss: 2.493206739425659\n",
      "loss: 2.4942636489868164\n",
      "loss: 2.569995403289795\n",
      "loss: 2.4853835105895996\n",
      "loss: 2.566364049911499\n",
      "loss: 2.5339133739471436\n",
      "loss: 2.5148167610168457\n",
      "loss: 2.4715118408203125\n",
      "loss: 2.4728825092315674\n",
      "loss: 2.5642144680023193\n",
      "loss: 2.4983272552490234\n",
      "loss: 2.4965603351593018\n",
      "loss: 2.531179428100586\n",
      "loss: 2.5721468925476074\n",
      "loss: 2.5091426372528076\n",
      "loss: 2.5759434700012207\n",
      "loss: 2.550781726837158\n",
      "loss: 2.4853127002716064\n",
      "loss: 2.48235821723938\n",
      "loss: 2.4051856994628906\n",
      "loss: 2.478794574737549\n",
      "loss: 2.441274881362915\n",
      "loss: 2.5977535247802734\n",
      "loss: 2.482358932495117\n",
      "loss: 2.5342235565185547\n",
      "loss: 2.5953786373138428\n",
      "loss: 2.44366717338562\n",
      "loss: 2.5944321155548096\n",
      "loss: 2.489934206008911\n",
      "loss: 2.4709277153015137\n",
      "loss: 2.5161571502685547\n",
      "loss: 2.620623826980591\n",
      "loss: 2.5875332355499268\n",
      "loss: 2.4518930912017822\n",
      "loss: 2.439981460571289\n",
      "loss: 2.4289495944976807\n",
      "loss: 2.477278470993042\n",
      "loss: 2.4773290157318115\n",
      "loss: 2.5061419010162354\n",
      "loss: 2.5309317111968994\n",
      "loss: 2.5856995582580566\n",
      "loss: 2.4720308780670166\n",
      "loss: 2.502711772918701\n",
      "loss: 2.5216403007507324\n",
      "loss: 2.3606066703796387\n",
      "loss: 2.566376209259033\n",
      "loss: 2.4628894329071045\n",
      "loss: 2.5876150131225586\n",
      "loss: 2.3522140979766846\n",
      "loss: 2.5107433795928955\n",
      "loss: 2.611379384994507\n",
      "loss: 2.455385208129883\n",
      "loss: 2.60471248626709\n",
      "loss: 2.4159231185913086\n",
      "loss: 2.539188861846924\n",
      "loss: 2.464148998260498\n",
      "loss: 2.4591941833496094\n",
      "loss: 2.557955265045166\n",
      "loss: 2.388582468032837\n",
      "loss: 2.453599691390991\n",
      "loss: 2.515801429748535\n",
      "loss: 2.6777689456939697\n",
      "loss: 2.586014747619629\n",
      "loss: 2.5011024475097656\n",
      "loss: 2.553225517272949\n",
      "loss: 2.3574531078338623\n",
      "loss: 2.4659578800201416\n",
      "loss: 2.5261850357055664\n",
      "loss: 2.472698926925659\n",
      "loss: 2.5188052654266357\n",
      "loss: 2.580859661102295\n",
      "loss: 2.6141326427459717\n",
      "loss: 2.4829440116882324\n",
      "loss: 2.4895145893096924\n",
      "loss: 2.593313694000244\n",
      "loss: 2.5657970905303955\n",
      "loss: 2.4454891681671143\n",
      "loss: 2.6444294452667236\n",
      "loss: 2.4959211349487305\n",
      "loss: 2.4419941902160645\n",
      "loss: 2.4846127033233643\n",
      "loss: 2.617544412612915\n",
      "loss: 2.6008083820343018\n",
      "loss: 2.547530174255371\n",
      "loss: 2.48944091796875\n",
      "loss: 2.6301162242889404\n",
      "loss: 2.4745655059814453\n",
      "loss: 2.452444076538086\n",
      "loss: 2.4564599990844727\n",
      "loss: 2.5420358180999756\n",
      "loss: 2.4971442222595215\n",
      "loss: 2.4963924884796143\n",
      "loss: 2.4703643321990967\n",
      "loss: 2.398005247116089\n",
      "loss: 2.481565475463867\n",
      "loss: 2.5323328971862793\n",
      "loss: 2.625905990600586\n",
      "loss: 2.605072498321533\n",
      "loss: 2.5166001319885254\n",
      "loss: 2.5555484294891357\n",
      "loss: 2.359894037246704\n",
      "loss: 2.4510059356689453\n",
      "loss: 2.514509916305542\n",
      "loss: 2.3682024478912354\n",
      "loss: 2.3328423500061035\n",
      "loss: 2.430516481399536\n",
      "loss: 2.4261698722839355\n",
      "loss: 2.5190186500549316\n",
      "loss: 2.5458054542541504\n",
      "loss: 2.439758539199829\n",
      "loss: 2.575972557067871\n",
      "loss: 2.5397658348083496\n",
      "loss: 2.487409830093384\n",
      "loss: 2.4130301475524902\n",
      "loss: 2.5645878314971924\n",
      "loss: 2.389761209487915\n",
      "loss: 2.3744451999664307\n",
      "loss: 2.4100892543792725\n",
      "loss: 2.383307695388794\n",
      "loss: 2.6005167961120605\n",
      "loss: 2.384799003601074\n",
      "loss: 2.693420648574829\n",
      "loss: 2.4112720489501953\n",
      "loss: 2.5562760829925537\n",
      "loss: 2.407133102416992\n",
      "loss: 2.4524590969085693\n",
      "loss: 2.6314947605133057\n",
      "loss: 2.4735560417175293\n",
      "loss: 2.4608118534088135\n",
      "loss: 2.5773909091949463\n",
      "loss: 2.440462827682495\n",
      "loss: 2.575011968612671\n",
      "loss: 2.4759440422058105\n",
      "loss: 2.5241575241088867\n",
      "loss: 2.4719204902648926\n",
      "loss: 2.5545802116394043\n",
      "loss: 2.3170721530914307\n",
      "loss: 2.5395708084106445\n",
      "loss: 2.439403772354126\n",
      "loss: 2.5091516971588135\n",
      "loss: 2.406532049179077\n",
      "loss: 2.4157474040985107\n",
      "loss: 2.6615405082702637\n",
      "loss: 2.447824001312256\n",
      "loss: 2.6194989681243896\n",
      "loss: 2.566701889038086\n",
      "loss: 2.572591781616211\n",
      "loss: 2.5372068881988525\n",
      "loss: 2.451437473297119\n",
      "loss: 2.4755449295043945\n",
      "loss: 2.5648763179779053\n",
      "loss: 2.5673632621765137\n",
      "loss: 2.5901939868927\n",
      "loss: 2.4632649421691895\n",
      "loss: 2.471348762512207\n",
      "loss: 2.5713396072387695\n",
      "loss: 2.60433030128479\n",
      "loss: 2.486269950866699\n",
      "loss: 2.494088888168335\n",
      "loss: 2.5719246864318848\n",
      "loss: 2.44926118850708\n",
      "loss: 2.4285807609558105\n",
      "loss: 2.591822385787964\n",
      "loss: 2.5036609172821045\n",
      "loss: 2.363952159881592\n",
      "loss: 2.4255237579345703\n",
      "loss: 2.450979709625244\n",
      "loss: 2.4782159328460693\n",
      "loss: 2.5685875415802\n",
      "loss: 2.5003607273101807\n",
      "loss: 2.5662410259246826\n",
      "loss: 2.512617349624634\n",
      "loss: 2.418637275695801\n",
      "loss: 2.547320604324341\n",
      "loss: 2.452254295349121\n",
      "loss: 2.522059440612793\n",
      "loss: 2.534912109375\n",
      "loss: 2.4579343795776367\n",
      "loss: 2.541348457336426\n",
      "loss: 2.4398367404937744\n",
      "loss: 2.477388858795166\n",
      "loss: 2.464897632598877\n",
      "loss: 2.51141357421875\n",
      "loss: 2.3409061431884766\n",
      "loss: 2.439006805419922\n",
      "loss: 2.5480077266693115\n",
      "loss: 2.531363010406494\n",
      "loss: 2.6071512699127197\n",
      "loss: 2.380936622619629\n",
      "loss: 2.5457825660705566\n",
      "loss: 2.5028538703918457\n",
      "loss: 2.5351006984710693\n",
      "loss: 2.4079372882843018\n",
      "loss: 2.46004319190979\n",
      "loss: 2.402414560317993\n",
      "loss: 2.5083694458007812\n",
      "loss: 2.4208338260650635\n",
      "loss: 2.4491500854492188\n",
      "loss: 2.504023313522339\n",
      "loss: 2.4211654663085938\n",
      "loss: 2.4360744953155518\n",
      "loss: 2.4647605419158936\n",
      "loss: 2.5434699058532715\n",
      "loss: 2.438993453979492\n",
      "loss: 2.529762029647827\n",
      "loss: 2.518962860107422\n",
      "loss: 2.4231908321380615\n",
      "loss: 2.4442925453186035\n",
      "loss: 2.4583017826080322\n",
      "loss: 2.5207064151763916\n",
      "loss: 2.53367018699646\n",
      "loss: 2.573143720626831\n",
      "loss: 2.424633026123047\n",
      "loss: 2.4746034145355225\n",
      "loss: 2.5198309421539307\n",
      "loss: 2.436047077178955\n",
      "loss: 2.4695937633514404\n",
      "loss: 2.53891921043396\n",
      "loss: 2.482883930206299\n",
      "loss: 2.5277860164642334\n",
      "loss: 2.4710710048675537\n",
      "loss: 2.5184712409973145\n",
      "loss: 2.319542169570923\n",
      "loss: 2.56730580329895\n",
      "loss: 2.503173589706421\n",
      "loss: 2.611351728439331\n",
      "loss: 2.4428348541259766\n",
      "loss: 2.4505789279937744\n",
      "loss: 2.4953715801239014\n",
      "loss: 2.3720221519470215\n",
      "loss: 2.6464715003967285\n",
      "loss: 2.357896327972412\n",
      "loss: 2.4353232383728027\n",
      "loss: 2.53546404838562\n",
      "loss: 2.5264315605163574\n",
      "loss: 2.4050633907318115\n",
      "loss: 2.4489083290100098\n",
      "loss: 2.533726692199707\n",
      "loss: 2.4501144886016846\n",
      "loss: 2.5158531665802\n",
      "loss: 2.4459848403930664\n",
      "loss: 2.448788642883301\n",
      "loss: 2.4746813774108887\n",
      "loss: 2.5042433738708496\n",
      "loss: 2.4941749572753906\n",
      "loss: 2.4941189289093018\n",
      "loss: 2.543147563934326\n",
      "loss: 2.5767223834991455\n",
      "loss: 2.549705743789673\n",
      "loss: 2.376706838607788\n",
      "loss: 2.6197428703308105\n",
      "loss: 2.624281167984009\n",
      "loss: 2.460258960723877\n",
      "loss: 2.504384756088257\n",
      "loss: 2.487517833709717\n",
      "loss: 2.5237741470336914\n",
      "loss: 2.465618133544922\n",
      "loss: 2.591747760772705\n",
      "loss: 2.497687578201294\n",
      "loss: 2.4928958415985107\n",
      "loss: 2.551536798477173\n",
      "loss: 2.6282224655151367\n",
      "loss: 2.43211030960083\n",
      "loss: 2.417544364929199\n",
      "loss: 2.431514263153076\n",
      "loss: 2.5122382640838623\n",
      "loss: 2.4218666553497314\n",
      "loss: 2.446209669113159\n",
      "loss: 2.446871042251587\n",
      "loss: 2.4881958961486816\n",
      "loss: 2.4478671550750732\n",
      "loss: 2.3628804683685303\n",
      "loss: 2.4388725757598877\n",
      "loss: 2.5602588653564453\n",
      "loss: 2.541072130203247\n",
      "loss: 2.4759116172790527\n",
      "loss: 2.5561747550964355\n",
      "loss: 2.5767662525177\n",
      "loss: 2.4949657917022705\n",
      "loss: 2.40649676322937\n",
      "loss: 2.316577196121216\n",
      "loss: 2.442531108856201\n",
      "loss: 2.4037766456604004\n",
      "loss: 2.42336368560791\n",
      "loss: 2.4724512100219727\n",
      "loss: 2.4191036224365234\n",
      "loss: 2.5332448482513428\n",
      "loss: 2.5812697410583496\n",
      "loss: 2.4698486328125\n",
      "loss: 2.576647996902466\n",
      "loss: 2.3690340518951416\n",
      "loss: 2.4310128688812256\n",
      "loss: 2.4638590812683105\n",
      "loss: 2.3798439502716064\n",
      "loss: 2.5228352546691895\n",
      "loss: 2.5956008434295654\n",
      "loss: 2.566378116607666\n",
      "loss: 2.5357825756073\n",
      "loss: 2.5255110263824463\n",
      "loss: 2.392693281173706\n",
      "loss: 2.4988975524902344\n",
      "loss: 2.5096898078918457\n",
      "loss: 2.475726366043091\n",
      "loss: 2.5319583415985107\n",
      "loss: 2.4180281162261963\n",
      "loss: 2.483583927154541\n",
      "loss: 2.416810989379883\n",
      "loss: 2.5472774505615234\n",
      "loss: 2.421123743057251\n",
      "loss: 2.3989241123199463\n",
      "loss: 2.450547456741333\n",
      "loss: 2.510838508605957\n",
      "loss: 2.5251963138580322\n",
      "loss: 2.5025463104248047\n",
      "loss: 2.508082389831543\n",
      "loss: 2.602285385131836\n",
      "loss: 2.4771382808685303\n",
      "loss: 2.4135854244232178\n",
      "loss: 2.4497764110565186\n",
      "loss: 2.452378511428833\n",
      "loss: 2.4469900131225586\n",
      "loss: 2.5567924976348877\n",
      "loss: 2.4879939556121826\n",
      "loss: 2.485520601272583\n",
      "loss: 2.3620519638061523\n",
      "loss: 2.6206138134002686\n",
      "loss: 2.3875370025634766\n",
      "loss: 2.4871342182159424\n",
      "loss: 2.5417661666870117\n",
      "loss: 2.4438862800598145\n",
      "loss: 2.4857165813446045\n",
      "loss: 2.3923003673553467\n",
      "loss: 2.3932008743286133\n",
      "loss: 2.4091832637786865\n",
      "loss: 2.4275546073913574\n",
      "loss: 2.454571008682251\n",
      "loss: 2.522235155105591\n",
      "loss: 2.5396721363067627\n",
      "loss: 2.471677303314209\n",
      "loss: 2.504885673522949\n",
      "loss: 2.448587656021118\n",
      "loss: 2.5438153743743896\n",
      "loss: 2.5461878776550293\n",
      "loss: 2.36460542678833\n",
      "loss: 2.3871116638183594\n",
      "loss: 2.5521767139434814\n",
      "loss: 2.497201919555664\n",
      "loss: 2.4374358654022217\n",
      "loss: 2.4198801517486572\n",
      "loss: 2.5010273456573486\n",
      "loss: 2.4198086261749268\n",
      "loss: 2.4973506927490234\n",
      "loss: 2.4879348278045654\n",
      "loss: 2.5551917552948\n",
      "loss: 2.4496662616729736\n",
      "loss: 2.3722212314605713\n",
      "loss: 2.4264707565307617\n",
      "loss: 2.4721503257751465\n",
      "loss: 2.4457836151123047\n",
      "loss: 2.525789260864258\n",
      "loss: 2.2783162593841553\n",
      "loss: 2.4324049949645996\n",
      "loss: 2.486875057220459\n",
      "loss: 2.4600417613983154\n",
      "loss: 2.4344289302825928\n",
      "loss: 2.5031871795654297\n",
      "loss: 2.51282000541687\n",
      "loss: 2.4699697494506836\n",
      "loss: 2.5173659324645996\n",
      "loss: 2.380662441253662\n",
      "loss: 2.4258713722229004\n",
      "loss: 2.528933048248291\n",
      "loss: 2.453047513961792\n",
      "loss: 2.537203550338745\n",
      "loss: 2.4696173667907715\n",
      "loss: 2.480756998062134\n",
      "loss: 2.4556286334991455\n",
      "loss: 2.483858585357666\n",
      "loss: 2.418915033340454\n",
      "loss: 2.4916326999664307\n",
      "loss: 2.529618263244629\n",
      "loss: 2.486696243286133\n",
      "loss: 2.611572742462158\n",
      "loss: 2.439424991607666\n",
      "loss: 2.452465057373047\n",
      "loss: 2.479727029800415\n",
      "loss: 2.5416648387908936\n",
      "loss: 2.51344633102417\n",
      "loss: 2.5090975761413574\n",
      "loss: 2.46836256980896\n",
      "loss: 2.4441585540771484\n",
      "loss: 2.5232248306274414\n",
      "loss: 2.4444692134857178\n",
      "loss: 2.379267930984497\n",
      "loss: 2.389277219772339\n",
      "loss: 2.536397695541382\n",
      "loss: 2.5140931606292725\n",
      "loss: 2.4130654335021973\n",
      "loss: 2.441298246383667\n",
      "loss: 2.4181900024414062\n",
      "loss: 2.5694522857666016\n",
      "loss: 2.473428249359131\n",
      "loss: 2.4400298595428467\n",
      "loss: 2.5856504440307617\n",
      "loss: 2.400613784790039\n",
      "loss: 2.37754225730896\n",
      "loss: 2.4815523624420166\n",
      "loss: 2.4644134044647217\n",
      "loss: 2.55165696144104\n",
      "loss: 2.428415298461914\n",
      "loss: 2.371422290802002\n",
      "loss: 2.4611051082611084\n",
      "loss: 2.4727046489715576\n",
      "loss: 2.6319243907928467\n",
      "loss: 2.4516103267669678\n",
      "loss: 2.5246434211730957\n",
      "loss: 2.327436685562134\n",
      "loss: 2.5736212730407715\n",
      "loss: 2.482888698577881\n",
      "loss: 2.5168895721435547\n",
      "loss: 2.449227809906006\n",
      "loss: 2.451014757156372\n",
      "loss: 2.5175423622131348\n",
      "loss: 2.501155376434326\n",
      "loss: 2.547813892364502\n",
      "loss: 2.4332876205444336\n",
      "loss: 2.429685592651367\n",
      "loss: 2.468759298324585\n",
      "loss: 2.5321502685546875\n",
      "loss: 2.53059720993042\n",
      "loss: 2.5103087425231934\n",
      "loss: 2.420745849609375\n",
      "loss: 2.4240574836730957\n",
      "loss: 2.5855462551116943\n",
      "loss: 2.565355062484741\n",
      "loss: 2.451653003692627\n",
      "loss: 2.5054266452789307\n",
      "loss: 2.5398337841033936\n",
      "loss: 2.4004580974578857\n",
      "loss: 2.447740077972412\n",
      "loss: 2.473566770553589\n",
      "loss: 2.530532121658325\n",
      "loss: 2.330617666244507\n",
      "loss: 2.4425439834594727\n",
      "loss: 2.2987096309661865\n",
      "loss: 2.5120391845703125\n",
      "loss: 2.473649263381958\n",
      "loss: 2.451610803604126\n",
      "loss: 2.348254680633545\n",
      "loss: 2.4695215225219727\n",
      "loss: 2.4317376613616943\n",
      "loss: 2.496772527694702\n",
      "loss: 2.4938297271728516\n",
      "loss: 2.5147242546081543\n",
      "loss: 2.4533791542053223\n",
      "loss: 2.628495216369629\n",
      "loss: 2.2861125469207764\n",
      "loss: 2.4459619522094727\n",
      "loss: 2.5671870708465576\n",
      "loss: 2.495058059692383\n",
      "loss: 2.428316354751587\n",
      "loss: 2.400485038757324\n",
      "loss: 2.5537314414978027\n",
      "loss: 2.3387649059295654\n",
      "loss: 2.455008029937744\n",
      "loss: 2.3406662940979004\n",
      "loss: 2.4333620071411133\n",
      "loss: 2.5870327949523926\n",
      "loss: 2.5243539810180664\n",
      "loss: 2.4938089847564697\n",
      "loss: 2.4287056922912598\n",
      "loss: 2.3815810680389404\n",
      "loss: 2.538113832473755\n",
      "loss: 2.381115674972534\n",
      "loss: 2.518181562423706\n",
      "loss: 2.463169813156128\n",
      "loss: 2.514329433441162\n",
      "loss: 2.4496185779571533\n",
      "loss: 2.5468969345092773\n",
      "loss: 2.500742197036743\n",
      "loss: 2.3854312896728516\n",
      "loss: 2.40548038482666\n",
      "loss: 2.5262222290039062\n",
      "loss: 2.4700756072998047\n",
      "loss: 2.497945547103882\n",
      "loss: 2.4021148681640625\n",
      "loss: 2.4570975303649902\n",
      "loss: 2.524477243423462\n",
      "loss: 2.450610399246216\n",
      "loss: 2.3995108604431152\n",
      "loss: 2.4997639656066895\n",
      "loss: 2.4192874431610107\n",
      "loss: 2.5814125537872314\n",
      "loss: 2.5580761432647705\n",
      "loss: 2.476428747177124\n",
      "loss: 2.37324857711792\n",
      "loss: 2.5105438232421875\n",
      "loss: 2.331235885620117\n",
      "loss: 2.4875574111938477\n",
      "loss: 2.487541437149048\n",
      "loss: 2.4335978031158447\n",
      "loss: 2.4309539794921875\n",
      "loss: 2.383716344833374\n",
      "loss: 2.401625156402588\n",
      "loss: 2.541443347930908\n",
      "loss: 2.3805112838745117\n",
      "loss: 2.39662766456604\n",
      "loss: 2.3728768825531006\n",
      "loss: 2.6414968967437744\n",
      "loss: 2.4113829135894775\n",
      "loss: 2.508127450942993\n",
      "loss: 2.52304744720459\n",
      "loss: 2.425477981567383\n",
      "loss: 2.3100955486297607\n",
      "loss: 2.494276523590088\n",
      "loss: 2.362313747406006\n",
      "loss: 2.4812350273132324\n",
      "loss: 2.320000171661377\n",
      "loss: 2.5328032970428467\n",
      "loss: 2.3633437156677246\n",
      "loss: 2.474541425704956\n",
      "loss: 2.4401354789733887\n",
      "loss: 2.5285556316375732\n",
      "loss: 2.5008251667022705\n",
      "loss: 2.419161796569824\n",
      "loss: 2.402968406677246\n",
      "loss: 2.5315914154052734\n",
      "loss: 2.467517852783203\n",
      "loss: 2.562802791595459\n",
      "loss: 2.4492664337158203\n",
      "loss: 2.5031630992889404\n",
      "loss: 2.483882188796997\n",
      "loss: 2.4174768924713135\n",
      "loss: 2.4526383876800537\n",
      "loss: 2.350433826446533\n",
      "loss: 2.5060431957244873\n",
      "loss: 2.6677989959716797\n",
      "loss: 2.3925461769104004\n",
      "loss: 2.451066493988037\n",
      "loss: 2.3608808517456055\n",
      "loss: 2.4499316215515137\n",
      "loss: 2.571516513824463\n",
      "loss: 2.5849106311798096\n",
      "loss: 2.5321576595306396\n",
      "loss: 2.5050296783447266\n",
      "loss: 2.468146562576294\n",
      "loss: 2.5568394660949707\n",
      "loss: 2.2708699703216553\n",
      "loss: 2.434743642807007\n",
      "loss: 2.3413820266723633\n",
      "loss: 2.3832828998565674\n",
      "loss: 2.391838550567627\n",
      "loss: 2.450317621231079\n",
      "loss: 2.585826873779297\n",
      "loss: 2.394073963165283\n",
      "loss: 2.469731092453003\n",
      "loss: 2.458589553833008\n",
      "loss: 2.5391664505004883\n",
      "loss: 2.3976593017578125\n",
      "loss: 2.5894832611083984\n",
      "loss: 2.5146021842956543\n",
      "loss: 2.5222597122192383\n",
      "loss: 2.525447130203247\n",
      "loss: 2.394832134246826\n",
      "loss: 2.4596662521362305\n",
      "loss: 2.469728946685791\n",
      "loss: 2.4550814628601074\n",
      "loss: 2.39880108833313\n",
      "loss: 2.506844997406006\n",
      "loss: 2.40523099899292\n",
      "loss: 2.532416343688965\n",
      "loss: 2.3770077228546143\n",
      "loss: 2.536370277404785\n",
      "loss: 2.508413791656494\n",
      "loss: 2.51611065864563\n",
      "loss: 2.4102227687835693\n",
      "loss: 2.5548698902130127\n",
      "loss: 2.5493321418762207\n",
      "loss: 2.4177086353302\n",
      "loss: 2.3687028884887695\n",
      "loss: 2.3725852966308594\n",
      "loss: 2.556077718734741\n",
      "loss: 2.3766672611236572\n",
      "loss: 2.523581027984619\n",
      "loss: 2.574627161026001\n",
      "loss: 2.468851327896118\n",
      "loss: 2.3734636306762695\n",
      "loss: 2.4956698417663574\n",
      "loss: 2.4662699699401855\n",
      "loss: 2.391629695892334\n",
      "loss: 2.567387104034424\n",
      "loss: 2.4499094486236572\n",
      "loss: 2.4915828704833984\n",
      "loss: 2.5071969032287598\n",
      "loss: 2.4611308574676514\n",
      "loss: 2.4221861362457275\n",
      "loss: 2.4512689113616943\n",
      "loss: 2.522456645965576\n",
      "loss: 2.3335342407226562\n",
      "loss: 2.420275926589966\n",
      "loss: 2.4278764724731445\n",
      "loss: 2.4934282302856445\n",
      "loss: 2.3137810230255127\n",
      "loss: 2.5278964042663574\n",
      "loss: 2.4582436084747314\n",
      "loss: 2.593895673751831\n",
      "loss: 2.466517210006714\n",
      "loss: 2.419290065765381\n",
      "loss: 2.3792881965637207\n",
      "loss: 2.4117438793182373\n",
      "loss: 2.440002918243408\n",
      "loss: 2.3776473999023438\n",
      "loss: 2.4256229400634766\n",
      "loss: 2.371903657913208\n",
      "loss: 2.367522716522217\n",
      "loss: 2.4964191913604736\n",
      "loss: 2.4073469638824463\n",
      "loss: 2.4055819511413574\n",
      "loss: 2.524371862411499\n",
      "loss: 2.4824697971343994\n",
      "loss: 2.561908483505249\n",
      "loss: 2.40091609954834\n",
      "loss: 2.4395222663879395\n",
      "loss: 2.4060628414154053\n",
      "loss: 2.366987466812134\n",
      "loss: 2.4285829067230225\n",
      "loss: 2.4268224239349365\n",
      "loss: 2.4343981742858887\n",
      "loss: 2.3877508640289307\n",
      "loss: 2.5566351413726807\n",
      "loss: 2.5798192024230957\n",
      "loss: 2.475287914276123\n",
      "loss: 2.6047470569610596\n",
      "loss: 2.497952699661255\n",
      "loss: 2.3195977210998535\n",
      "loss: 2.5043704509735107\n",
      "loss: 2.4300222396850586\n",
      "loss: 2.3974568843841553\n",
      "loss: 2.4771406650543213\n",
      "loss: 2.5105223655700684\n",
      "loss: 2.3773083686828613\n",
      "loss: 2.585878372192383\n",
      "loss: 2.4568896293640137\n",
      "loss: 2.537806749343872\n",
      "loss: 2.448850393295288\n",
      "loss: 2.3571786880493164\n",
      "loss: 2.5335278511047363\n",
      "loss: 2.4895710945129395\n",
      "loss: 2.45668888092041\n",
      "loss: 2.4093880653381348\n",
      "loss: 2.570319414138794\n",
      "loss: 2.4897756576538086\n",
      "loss: 2.5540432929992676\n",
      "loss: 2.507753610610962\n",
      "loss: 2.457887887954712\n",
      "loss: 2.5646214485168457\n",
      "loss: 2.479349136352539\n",
      "loss: 2.36914324760437\n",
      "loss: 2.5542500019073486\n",
      "loss: 2.4595625400543213\n",
      "loss: 2.572751522064209\n",
      "loss: 2.597524881362915\n",
      "loss: 2.4415438175201416\n",
      "loss: 2.4073023796081543\n",
      "loss: 2.4091053009033203\n",
      "loss: 2.361254930496216\n",
      "loss: 2.3610286712646484\n",
      "loss: 2.416018486022949\n",
      "loss: 2.622443199157715\n",
      "loss: 2.4661865234375\n",
      "loss: 2.345684289932251\n",
      "loss: 2.3641371726989746\n",
      "loss: 2.4590542316436768\n",
      "loss: 2.487652540206909\n",
      "loss: 2.3795247077941895\n",
      "loss: 2.42231822013855\n",
      "loss: 2.465406894683838\n",
      "loss: 2.5564444065093994\n",
      "loss: 2.5562517642974854\n",
      "loss: 2.375199794769287\n",
      "loss: 2.579010009765625\n",
      "loss: 2.5505459308624268\n",
      "loss: 2.42887806892395\n",
      "loss: 2.4255239963531494\n",
      "loss: 2.4844648838043213\n",
      "loss: 2.437985897064209\n",
      "loss: 2.346010446548462\n",
      "loss: 2.4888834953308105\n",
      "loss: 2.37904953956604\n",
      "loss: 2.500792980194092\n",
      "loss: 2.4203341007232666\n",
      "loss: 2.453704833984375\n",
      "loss: 2.6314525604248047\n",
      "loss: 2.230548620223999\n",
      "loss: 2.3899102210998535\n",
      "loss: 2.4327781200408936\n",
      "loss: 2.4333698749542236\n",
      "loss: 2.3355305194854736\n",
      "loss: 2.341885566711426\n",
      "loss: 2.4054641723632812\n",
      "loss: 2.5503101348876953\n",
      "loss: 2.464247226715088\n",
      "loss: 2.5214688777923584\n",
      "loss: 2.517808437347412\n",
      "loss: 2.363312005996704\n",
      "loss: 2.4698665142059326\n",
      "loss: 2.529803514480591\n",
      "loss: 2.497335195541382\n",
      "loss: 2.342397689819336\n",
      "loss: 2.348123788833618\n",
      "loss: 2.39654541015625\n",
      "loss: 2.4880237579345703\n",
      "loss: 2.5585741996765137\n",
      "loss: 2.469454288482666\n",
      "loss: 2.4257328510284424\n",
      "loss: 2.370436906814575\n",
      "loss: 2.3961799144744873\n",
      "loss: 2.4385900497436523\n",
      "loss: 2.594263792037964\n",
      "loss: 2.4374797344207764\n",
      "loss: 2.482588529586792\n",
      "loss: 2.5346744060516357\n",
      "loss: 2.3975155353546143\n",
      "loss: 2.483431816101074\n",
      "loss: 2.4120819568634033\n",
      "loss: 2.6167187690734863\n",
      "loss: 2.4760475158691406\n",
      "loss: 2.47365140914917\n",
      "loss: 2.503587245941162\n",
      "loss: 2.423807144165039\n",
      "loss: 2.4588963985443115\n",
      "loss: 2.5160932540893555\n",
      "loss: 2.488654375076294\n",
      "loss: 2.5329246520996094\n",
      "loss: 2.53147554397583\n",
      "loss: 2.532965660095215\n",
      "loss: 2.5259368419647217\n",
      "loss: 2.425151824951172\n",
      "loss: 2.5979487895965576\n",
      "loss: 2.482841968536377\n",
      "loss: 2.377502202987671\n",
      "loss: 2.4016880989074707\n",
      "loss: 2.5204834938049316\n",
      "loss: 2.53725266456604\n",
      "loss: 2.4016218185424805\n",
      "loss: 2.357670783996582\n",
      "loss: 2.4726274013519287\n",
      "loss: 2.39717960357666\n",
      "loss: 2.474396228790283\n",
      "loss: 2.3450138568878174\n",
      "loss: 2.4229469299316406\n",
      "loss: 2.372549057006836\n",
      "loss: 2.3549447059631348\n",
      "loss: 2.4142777919769287\n",
      "loss: 2.3447041511535645\n",
      "loss: 2.4047536849975586\n",
      "loss: 2.488614559173584\n",
      "loss: 2.5241281986236572\n",
      "loss: 2.5206661224365234\n",
      "loss: 2.36198353767395\n",
      "loss: 2.448993682861328\n",
      "loss: 2.625633955001831\n",
      "loss: 2.5352489948272705\n",
      "loss: 2.4585843086242676\n",
      "loss: 2.460378408432007\n",
      "loss: 2.5003912448883057\n",
      "loss: 2.372246026992798\n",
      "loss: 2.341540813446045\n",
      "loss: 2.3879308700561523\n",
      "loss: 2.430241584777832\n",
      "loss: 2.4498744010925293\n",
      "loss: 2.6124768257141113\n",
      "loss: 2.5073630809783936\n",
      "loss: 2.3740463256835938\n",
      "loss: 2.4895825386047363\n",
      "loss: 2.3943140506744385\n",
      "loss: 2.4207041263580322\n",
      "loss: 2.537363290786743\n",
      "loss: 2.4178884029388428\n",
      "loss: 2.4365596771240234\n",
      "loss: 2.559931993484497\n",
      "loss: 2.4992730617523193\n",
      "loss: 2.492222309112549\n",
      "loss: 2.539613723754883\n",
      "loss: 2.4592669010162354\n",
      "loss: 2.593087911605835\n",
      "loss: 2.443012237548828\n",
      "loss: 2.5015673637390137\n",
      "loss: 2.4843597412109375\n",
      "loss: 2.425452947616577\n",
      "loss: 2.418212890625\n",
      "loss: 2.60850191116333\n",
      "loss: 2.4642510414123535\n",
      "loss: 2.4156529903411865\n",
      "loss: 2.49743390083313\n",
      "loss: 2.402613878250122\n",
      "loss: 2.5219149589538574\n",
      "loss: 2.468254327774048\n",
      "loss: 2.4680776596069336\n",
      "loss: 2.4080889225006104\n",
      "loss: 2.4044291973114014\n",
      "loss: 2.4911088943481445\n",
      "loss: 2.57590913772583\n",
      "loss: 2.476857900619507\n",
      "loss: 2.662670373916626\n",
      "loss: 2.468087911605835\n",
      "loss: 2.3487744331359863\n",
      "loss: 2.329397439956665\n",
      "loss: 2.363002061843872\n",
      "loss: 2.521724224090576\n",
      "loss: 2.378073215484619\n",
      "loss: 2.4264132976531982\n",
      "loss: 2.441039800643921\n",
      "loss: 2.5351638793945312\n",
      "loss: 2.433980703353882\n",
      "loss: 2.499006986618042\n",
      "loss: 2.4556777477264404\n",
      "loss: 2.375054121017456\n",
      "loss: 2.485712766647339\n",
      "loss: 2.425795316696167\n",
      "loss: 2.456590414047241\n",
      "loss: 2.601466178894043\n",
      "loss: 2.325495958328247\n",
      "loss: 2.43215274810791\n",
      "loss: 2.4125587940216064\n",
      "loss: 2.5020201206207275\n",
      "loss: 2.3971943855285645\n",
      "loss: 2.4765255451202393\n",
      "loss: 2.4524807929992676\n",
      "loss: 2.4641594886779785\n",
      "loss: 2.5021872520446777\n",
      "loss: 2.623173475265503\n",
      "loss: 2.4440722465515137\n",
      "loss: 2.4221885204315186\n",
      "loss: 2.512681484222412\n",
      "loss: 2.5458295345306396\n",
      "loss: 2.2937047481536865\n",
      "loss: 2.3672657012939453\n",
      "loss: 2.494553327560425\n",
      "loss: 2.399355888366699\n",
      "loss: 2.5621626377105713\n",
      "loss: 2.333893299102783\n",
      "loss: 2.557770013809204\n",
      "loss: 2.523102045059204\n",
      "loss: 2.504725456237793\n",
      "loss: 2.595283269882202\n",
      "loss: 2.3511781692504883\n",
      "loss: 2.398400068283081\n",
      "loss: 2.5236480236053467\n",
      "loss: 2.4245169162750244\n",
      "loss: 2.4275918006896973\n",
      "loss: 2.6166722774505615\n",
      "loss: 2.3925461769104004\n",
      "loss: 2.4584925174713135\n",
      "loss: 2.4414427280426025\n",
      "loss: 2.422020673751831\n",
      "loss: 2.4283945560455322\n",
      "loss: 2.3558590412139893\n",
      "loss: 2.576246738433838\n",
      "loss: 2.37919545173645\n",
      "loss: 2.479562997817993\n",
      "loss: 2.478137731552124\n",
      "loss: 2.434338331222534\n",
      "loss: 2.387422800064087\n",
      "loss: 2.434540271759033\n",
      "loss: 2.490098237991333\n",
      "loss: 2.5338470935821533\n",
      "loss: 2.3727164268493652\n",
      "loss: 2.5022730827331543\n",
      "loss: 2.5016918182373047\n",
      "loss: 2.527015209197998\n",
      "loss: 2.3845717906951904\n",
      "loss: 2.425518035888672\n",
      "loss: 2.5342516899108887\n",
      "loss: 2.4273407459259033\n",
      "loss: 2.5502758026123047\n",
      "loss: 2.4258036613464355\n",
      "loss: 2.349440336227417\n",
      "loss: 2.5152173042297363\n",
      "loss: 2.5021908283233643\n",
      "loss: 2.480403423309326\n",
      "loss: 2.4549636840820312\n",
      "loss: 2.379476547241211\n",
      "loss: 2.4831383228302\n",
      "loss: 2.404250144958496\n",
      "loss: 2.3879475593566895\n",
      "loss: 2.482379913330078\n",
      "loss: 2.4610326290130615\n",
      "loss: 2.3334994316101074\n",
      "loss: 2.543560266494751\n",
      "loss: 2.479370355606079\n",
      "loss: 2.4686532020568848\n",
      "loss: 2.509488105773926\n",
      "loss: 2.427900791168213\n",
      "loss: 2.5906949043273926\n",
      "loss: 2.5520739555358887\n",
      "loss: 2.3087775707244873\n",
      "loss: 2.4445650577545166\n",
      "loss: 2.4546709060668945\n",
      "loss: 2.4519591331481934\n",
      "loss: 2.439887046813965\n",
      "loss: 2.4306704998016357\n",
      "loss: 2.491187334060669\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 10 == 0:\n",
    "        print(f'loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fofur af II dwid ty yot thevaustome f preay ld, s t nchanthinf an k healoblldond hininche th ld is.\n",
      "\n",
      "Thye hatst thus\n",
      "Ou? mespriowh codyove d.\n",
      "I nete theiapoba wheinsothe Cord sithine p lfulther, wallofe t ICo:\n",
      "ME:\n",
      "OUMERLO:\n",
      "NI wayouroikl y ftwaveat he'\n",
      "St H:\n",
      "Lurthou hefamy ipont y te y py wide. th ce br mistorasthame Oust orinkeal qureribot is ddy nechthon m t; ghen than.\n",
      "HMI al ind mprs thie!\n",
      "Wed rur,-d, therothanowesll g t betert d?\n",
      "ANonjunfu e ting nd, whefacowoke cowh ds,\n",
      "O:\n",
      "Fo che te, IUSepeliformarrorank mincourecharenve awh s,\n",
      "GLAnct, INams wimuthar\n",
      "WAUSS:\n",
      "Bur tis ey ou rssensoshorswin wesse nd l fe oninor y macinee therouneme atho or wise indo, t rn asllianeedend ctr me?\n",
      "wheint t p, ch lindisearouis tl'so I therowile hingrthad\n",
      "ORULLLUTI a mato;\n",
      "WBRino beay IUS:\n",
      "Whowofrd s athatsond ousthiflad d oul t f ime, am he: iche\n",
      "\n",
      "AMos, k wherman de fam ban y o ing IDWhend mey, k,\n",
      "ORICo ou fuswais theforobjusmen, ma,\n",
      "WAlas\n",
      "Datrncrayord.\n",
      "TRD orme!\n",
      "Wherinthal d cris,\n",
      "Buss b\n",
      "Whoy I r n d shey\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "gen_seq = m.generate(idx, max_new_tokens=1000)\n",
    "print(decode(gen_seq[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
