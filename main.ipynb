{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-18 12:38:48--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1,1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1,06M  6,55MB/s    in 0,2s    \n",
      "\n",
      "2024-07-18 12:38:49 (6,55 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of text: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size:  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) then target is 47\n",
      "when input is tensor([18, 47]) then target is 56\n",
      "when input is tensor([18, 47, 56]) then target is 57\n",
      "when input is tensor([18, 47, 56, 57]) then target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) then target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) then target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) then target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} then target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8]) torch.int64\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:  torch.Size([4, 8]) torch.int64\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] then target is 43\n",
      "when input is [24, 43] then target is 58\n",
      "when input is [24, 43, 58] then target is 5\n",
      "when input is [24, 43, 58, 5] then target is 57\n",
      "when input is [24, 43, 58, 5, 57] then target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] then target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] then target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] then target is 39\n",
      "when input is [44] then target is 53\n",
      "when input is [44, 53] then target is 56\n",
      "when input is [44, 53, 56] then target is 1\n",
      "when input is [44, 53, 56, 1] then target is 58\n",
      "when input is [44, 53, 56, 1, 58] then target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] then target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] then target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] then target is 1\n",
      "when input is [52] then target is 58\n",
      "when input is [52, 58] then target is 1\n",
      "when input is [52, 58, 1] then target is 58\n",
      "when input is [52, 58, 1, 58] then target is 46\n",
      "when input is [52, 58, 1, 58, 46] then target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] then target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] then target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] then target is 46\n",
      "when input is [25] then target is 17\n",
      "when input is [25, 17] then target is 27\n",
      "when input is [25, 17, 27] then target is 10\n",
      "when input is [25, 17, 27, 10] then target is 0\n",
      "when input is [25, 17, 27, 10, 0] then target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] then target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] then target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] then target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(data.size(0) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + 1 + block_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs: ', xb.shape, xb.dtype)\n",
    "print(xb)\n",
    "print('targets: ', yb.shape, yb.dtype)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} then target is {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "gen_seq = m.generate(idx, max_new_tokens=100)\n",
    "print(decode(gen_seq[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.692410945892334\n",
      "loss: 4.71571159362793\n",
      "loss: 4.630363464355469\n",
      "loss: 4.722168922424316\n",
      "loss: 4.737388610839844\n",
      "loss: 4.509944915771484\n",
      "loss: 4.707157135009766\n",
      "loss: 4.62866735458374\n",
      "loss: 4.599071502685547\n",
      "loss: 4.646826267242432\n",
      "loss: 4.6221184730529785\n",
      "loss: 4.569244384765625\n",
      "loss: 4.594120979309082\n",
      "loss: 4.4772257804870605\n",
      "loss: 4.722170829772949\n",
      "loss: 4.495169162750244\n",
      "loss: 4.606575012207031\n",
      "loss: 4.496219158172607\n",
      "loss: 4.5546135902404785\n",
      "loss: 4.523606777191162\n",
      "loss: 4.551448345184326\n",
      "loss: 4.432989120483398\n",
      "loss: 4.422276020050049\n",
      "loss: 4.420462131500244\n",
      "loss: 4.40394401550293\n",
      "loss: 4.497410297393799\n",
      "loss: 4.333616256713867\n",
      "loss: 4.353393077850342\n",
      "loss: 4.382663249969482\n",
      "loss: 4.440627098083496\n",
      "loss: 4.348025321960449\n",
      "loss: 4.349501132965088\n",
      "loss: 4.360719203948975\n",
      "loss: 4.409273624420166\n",
      "loss: 4.312803268432617\n",
      "loss: 4.240131855010986\n",
      "loss: 4.183422088623047\n",
      "loss: 4.347282886505127\n",
      "loss: 4.267045497894287\n",
      "loss: 4.197660446166992\n",
      "loss: 4.258787155151367\n",
      "loss: 4.394116401672363\n",
      "loss: 4.200348854064941\n",
      "loss: 4.235145568847656\n",
      "loss: 4.3049116134643555\n",
      "loss: 4.224520683288574\n",
      "loss: 4.119238376617432\n",
      "loss: 4.2600178718566895\n",
      "loss: 4.159659385681152\n",
      "loss: 4.143760681152344\n",
      "loss: 4.218249797821045\n",
      "loss: 4.098305702209473\n",
      "loss: 4.2440409660339355\n",
      "loss: 4.215926170349121\n",
      "loss: 4.13374137878418\n",
      "loss: 4.049482345581055\n",
      "loss: 4.162442684173584\n",
      "loss: 4.122520923614502\n",
      "loss: 4.129222393035889\n",
      "loss: 4.1966552734375\n",
      "loss: 4.128293037414551\n",
      "loss: 4.061880588531494\n",
      "loss: 4.04954195022583\n",
      "loss: 4.185947895050049\n",
      "loss: 3.9805314540863037\n",
      "loss: 4.01810359954834\n",
      "loss: 4.035335540771484\n",
      "loss: 4.071683406829834\n",
      "loss: 3.9107062816619873\n",
      "loss: 4.08574914932251\n",
      "loss: 3.9904613494873047\n",
      "loss: 3.9406449794769287\n",
      "loss: 4.001192569732666\n",
      "loss: 3.8991920948028564\n",
      "loss: 3.9660868644714355\n",
      "loss: 3.9408321380615234\n",
      "loss: 4.00991153717041\n",
      "loss: 3.963670015335083\n",
      "loss: 3.921130418777466\n",
      "loss: 3.799042224884033\n",
      "loss: 3.9563486576080322\n",
      "loss: 3.8749523162841797\n",
      "loss: 3.8201346397399902\n",
      "loss: 3.8448939323425293\n",
      "loss: 3.8063318729400635\n",
      "loss: 3.935490608215332\n",
      "loss: 3.776577949523926\n",
      "loss: 3.8508589267730713\n",
      "loss: 3.881897211074829\n",
      "loss: 3.8443572521209717\n",
      "loss: 3.8422818183898926\n",
      "loss: 3.7702889442443848\n",
      "loss: 3.7822747230529785\n",
      "loss: 3.7615482807159424\n",
      "loss: 3.8587985038757324\n",
      "loss: 3.9218997955322266\n",
      "loss: 3.709153413772583\n",
      "loss: 3.72884464263916\n",
      "loss: 3.7445390224456787\n",
      "loss: 3.7521514892578125\n",
      "loss: 3.7683708667755127\n",
      "loss: 3.7500436305999756\n",
      "loss: 3.6567132472991943\n",
      "loss: 3.7909092903137207\n",
      "loss: 3.736635208129883\n",
      "loss: 3.742079973220825\n",
      "loss: 3.6173789501190186\n",
      "loss: 3.699249505996704\n",
      "loss: 3.6355950832366943\n",
      "loss: 3.6888623237609863\n",
      "loss: 3.6872823238372803\n",
      "loss: 3.693242073059082\n",
      "loss: 3.5396785736083984\n",
      "loss: 3.534531354904175\n",
      "loss: 3.6011064052581787\n",
      "loss: 3.596647024154663\n",
      "loss: 3.6019692420959473\n",
      "loss: 3.701012134552002\n",
      "loss: 3.5401642322540283\n",
      "loss: 3.6790289878845215\n",
      "loss: 3.5369598865509033\n",
      "loss: 3.4836771488189697\n",
      "loss: 3.5324087142944336\n",
      "loss: 3.5624051094055176\n",
      "loss: 3.7032032012939453\n",
      "loss: 3.4596331119537354\n",
      "loss: 3.590540647506714\n",
      "loss: 3.5485503673553467\n",
      "loss: 3.4802937507629395\n",
      "loss: 3.5416011810302734\n",
      "loss: 3.5174341201782227\n",
      "loss: 3.458162546157837\n",
      "loss: 3.479227066040039\n",
      "loss: 3.4692816734313965\n",
      "loss: 3.56852126121521\n",
      "loss: 3.5019760131835938\n",
      "loss: 3.6115620136260986\n",
      "loss: 3.4493603706359863\n",
      "loss: 3.4705536365509033\n",
      "loss: 3.4477972984313965\n",
      "loss: 3.500946283340454\n",
      "loss: 3.419642210006714\n",
      "loss: 3.401430368423462\n",
      "loss: 3.415173053741455\n",
      "loss: 3.5123705863952637\n",
      "loss: 3.331974744796753\n",
      "loss: 3.4438767433166504\n",
      "loss: 3.4583635330200195\n",
      "loss: 3.3440070152282715\n",
      "loss: 3.3879973888397217\n",
      "loss: 3.3405673503875732\n",
      "loss: 3.405850648880005\n",
      "loss: 3.3964858055114746\n",
      "loss: 3.3966219425201416\n",
      "loss: 3.3163883686065674\n",
      "loss: 3.3197202682495117\n",
      "loss: 3.2522099018096924\n",
      "loss: 3.3601133823394775\n",
      "loss: 3.4180972576141357\n",
      "loss: 3.337883710861206\n",
      "loss: 3.3707377910614014\n",
      "loss: 3.2687718868255615\n",
      "loss: 3.3806254863739014\n",
      "loss: 3.304457187652588\n",
      "loss: 3.27360200881958\n",
      "loss: 3.358269453048706\n",
      "loss: 3.2949745655059814\n",
      "loss: 3.2897536754608154\n",
      "loss: 3.2096056938171387\n",
      "loss: 3.297935724258423\n",
      "loss: 3.285342216491699\n",
      "loss: 3.142754554748535\n",
      "loss: 3.3214571475982666\n",
      "loss: 3.2053022384643555\n",
      "loss: 3.2200124263763428\n",
      "loss: 3.2061288356781006\n",
      "loss: 3.1577558517456055\n",
      "loss: 3.2135021686553955\n",
      "loss: 3.2014687061309814\n",
      "loss: 3.241170644760132\n",
      "loss: 3.1336934566497803\n",
      "loss: 3.3265814781188965\n",
      "loss: 3.2029097080230713\n",
      "loss: 3.254757881164551\n",
      "loss: 3.2152411937713623\n",
      "loss: 3.095712184906006\n",
      "loss: 3.2713005542755127\n",
      "loss: 3.1061766147613525\n",
      "loss: 3.1515250205993652\n",
      "loss: 3.252741813659668\n",
      "loss: 3.1624464988708496\n",
      "loss: 3.2475531101226807\n",
      "loss: 3.190401077270508\n",
      "loss: 3.2756974697113037\n",
      "loss: 3.1720728874206543\n",
      "loss: 3.1327338218688965\n",
      "loss: 3.1884689331054688\n",
      "loss: 3.072434425354004\n",
      "loss: 3.1947836875915527\n",
      "loss: 3.1323676109313965\n",
      "loss: 3.2364423274993896\n",
      "loss: 3.247915744781494\n",
      "loss: 3.0953311920166016\n",
      "loss: 3.138178586959839\n",
      "loss: 3.024836778640747\n",
      "loss: 3.1373777389526367\n",
      "loss: 3.1482748985290527\n",
      "loss: 3.015535593032837\n",
      "loss: 2.980320930480957\n",
      "loss: 3.1267099380493164\n",
      "loss: 2.9984419345855713\n",
      "loss: 2.9984614849090576\n",
      "loss: 3.0655930042266846\n",
      "loss: 3.0285141468048096\n",
      "loss: 3.1223480701446533\n",
      "loss: 3.060685873031616\n",
      "loss: 3.022596597671509\n",
      "loss: 3.149158477783203\n",
      "loss: 3.124237060546875\n",
      "loss: 3.1492958068847656\n",
      "loss: 3.0962109565734863\n",
      "loss: 3.085139274597168\n",
      "loss: 3.031507968902588\n",
      "loss: 3.019059419631958\n",
      "loss: 2.9581778049468994\n",
      "loss: 2.9819514751434326\n",
      "loss: 3.002634048461914\n",
      "loss: 3.1182703971862793\n",
      "loss: 3.0930681228637695\n",
      "loss: 2.875169515609741\n",
      "loss: 2.9782586097717285\n",
      "loss: 3.0366668701171875\n",
      "loss: 3.042264223098755\n",
      "loss: 3.0569231510162354\n",
      "loss: 2.9356820583343506\n",
      "loss: 2.9739267826080322\n",
      "loss: 3.037306785583496\n",
      "loss: 3.026608467102051\n",
      "loss: 3.052182912826538\n",
      "loss: 2.9475667476654053\n",
      "loss: 2.8903520107269287\n",
      "loss: 2.952566623687744\n",
      "loss: 2.975797176361084\n",
      "loss: 3.0735199451446533\n",
      "loss: 3.0350098609924316\n",
      "loss: 2.9756391048431396\n",
      "loss: 2.992076873779297\n",
      "loss: 2.9166574478149414\n",
      "loss: 3.0616750717163086\n",
      "loss: 2.95859432220459\n",
      "loss: 2.9388511180877686\n",
      "loss: 2.9829306602478027\n",
      "loss: 3.02652645111084\n",
      "loss: 2.9102671146392822\n",
      "loss: 2.9478461742401123\n",
      "loss: 2.9767067432403564\n",
      "loss: 2.9527342319488525\n",
      "loss: 2.9513182640075684\n",
      "loss: 2.965742826461792\n",
      "loss: 2.746842384338379\n",
      "loss: 2.8247711658477783\n",
      "loss: 2.9029808044433594\n",
      "loss: 2.8629181385040283\n",
      "loss: 2.8373897075653076\n",
      "loss: 2.9195363521575928\n",
      "loss: 2.8646535873413086\n",
      "loss: 3.0753438472747803\n",
      "loss: 2.9632556438446045\n",
      "loss: 2.9275319576263428\n",
      "loss: 3.007578134536743\n",
      "loss: 2.920940637588501\n",
      "loss: 2.822652578353882\n",
      "loss: 2.8927085399627686\n",
      "loss: 2.9639909267425537\n",
      "loss: 2.935136556625366\n",
      "loss: 2.7931432723999023\n",
      "loss: 2.9825189113616943\n",
      "loss: 2.8665685653686523\n",
      "loss: 2.8064475059509277\n",
      "loss: 2.8550961017608643\n",
      "loss: 2.8865857124328613\n",
      "loss: 2.7618510723114014\n",
      "loss: 2.78377366065979\n",
      "loss: 2.7391510009765625\n",
      "loss: 2.896890640258789\n",
      "loss: 2.8618054389953613\n",
      "loss: 2.9348793029785156\n",
      "loss: 2.878087282180786\n",
      "loss: 2.7936341762542725\n",
      "loss: 2.8117125034332275\n",
      "loss: 2.8703949451446533\n",
      "loss: 2.818065643310547\n",
      "loss: 2.8087663650512695\n",
      "loss: 2.7873878479003906\n",
      "loss: 2.6851089000701904\n",
      "loss: 2.849501609802246\n",
      "loss: 2.7754921913146973\n",
      "loss: 2.7849137783050537\n",
      "loss: 2.7837679386138916\n",
      "loss: 2.762500524520874\n",
      "loss: 2.892672061920166\n",
      "loss: 2.9027013778686523\n",
      "loss: 2.8386802673339844\n",
      "loss: 2.915477752685547\n",
      "loss: 2.80234956741333\n",
      "loss: 2.7046427726745605\n",
      "loss: 2.735935688018799\n",
      "loss: 2.8291890621185303\n",
      "loss: 2.7533934116363525\n",
      "loss: 2.676922559738159\n",
      "loss: 2.754257917404175\n",
      "loss: 2.673513174057007\n",
      "loss: 2.7304840087890625\n",
      "loss: 2.841806650161743\n",
      "loss: 2.852717161178589\n",
      "loss: 2.796854257583618\n",
      "loss: 2.7533791065216064\n",
      "loss: 2.8604679107666016\n",
      "loss: 2.7396597862243652\n",
      "loss: 2.7263972759246826\n",
      "loss: 2.596656084060669\n",
      "loss: 2.792116403579712\n",
      "loss: 2.946808099746704\n",
      "loss: 2.6978495121002197\n",
      "loss: 2.636988878250122\n",
      "loss: 2.755399703979492\n",
      "loss: 2.8020858764648438\n",
      "loss: 2.752257823944092\n",
      "loss: 2.6976211071014404\n",
      "loss: 2.7613720893859863\n",
      "loss: 2.625274419784546\n",
      "loss: 2.7543954849243164\n",
      "loss: 2.7430408000946045\n",
      "loss: 2.8059260845184326\n",
      "loss: 2.635920524597168\n",
      "loss: 2.842974901199341\n",
      "loss: 2.8451879024505615\n",
      "loss: 2.7050936222076416\n",
      "loss: 2.7090811729431152\n",
      "loss: 2.5857019424438477\n",
      "loss: 2.7114689350128174\n",
      "loss: 2.750986099243164\n",
      "loss: 2.7808032035827637\n",
      "loss: 2.699094533920288\n",
      "loss: 2.7598469257354736\n",
      "loss: 2.661818027496338\n",
      "loss: 2.6426353454589844\n",
      "loss: 2.655503273010254\n",
      "loss: 2.627866268157959\n",
      "loss: 2.738431692123413\n",
      "loss: 2.717909336090088\n",
      "loss: 2.6530842781066895\n",
      "loss: 2.705747365951538\n",
      "loss: 2.7289392948150635\n",
      "loss: 2.628253936767578\n",
      "loss: 2.6381053924560547\n",
      "loss: 2.727670431137085\n",
      "loss: 2.7590198516845703\n",
      "loss: 2.809558868408203\n",
      "loss: 2.7340190410614014\n",
      "loss: 2.7127628326416016\n",
      "loss: 2.577857732772827\n",
      "loss: 2.57894229888916\n",
      "loss: 2.6795992851257324\n",
      "loss: 2.564098834991455\n",
      "loss: 2.7606053352355957\n",
      "loss: 2.6938655376434326\n",
      "loss: 2.6534531116485596\n",
      "loss: 2.646381378173828\n",
      "loss: 2.6373307704925537\n",
      "loss: 2.6028945446014404\n",
      "loss: 2.704761266708374\n",
      "loss: 2.5402469635009766\n",
      "loss: 2.6234593391418457\n",
      "loss: 2.609485149383545\n",
      "loss: 2.54787015914917\n",
      "loss: 2.7466447353363037\n",
      "loss: 2.5768988132476807\n",
      "loss: 2.629389762878418\n",
      "loss: 2.702894449234009\n",
      "loss: 2.7221789360046387\n",
      "loss: 2.7454957962036133\n",
      "loss: 2.6684505939483643\n",
      "loss: 2.7380738258361816\n",
      "loss: 2.629412889480591\n",
      "loss: 2.6111373901367188\n",
      "loss: 2.657222032546997\n",
      "loss: 2.639462947845459\n",
      "loss: 2.744905471801758\n",
      "loss: 2.6258842945098877\n",
      "loss: 2.604307174682617\n",
      "loss: 2.655421495437622\n",
      "loss: 2.7265548706054688\n",
      "loss: 2.610083818435669\n",
      "loss: 2.6680405139923096\n",
      "loss: 2.7071645259857178\n",
      "loss: 2.6605727672576904\n",
      "loss: 2.5937588214874268\n",
      "loss: 2.7415449619293213\n",
      "loss: 2.664018392562866\n",
      "loss: 2.70308780670166\n",
      "loss: 2.6568872928619385\n",
      "loss: 2.6355533599853516\n",
      "loss: 2.7546756267547607\n",
      "loss: 2.54518461227417\n",
      "loss: 2.6806094646453857\n",
      "loss: 2.6001060009002686\n",
      "loss: 2.6349081993103027\n",
      "loss: 2.5764803886413574\n",
      "loss: 2.6965222358703613\n",
      "loss: 2.7401247024536133\n",
      "loss: 2.64658522605896\n",
      "loss: 2.7291817665100098\n",
      "loss: 2.611990451812744\n",
      "loss: 2.6678597927093506\n",
      "loss: 2.6046342849731445\n",
      "loss: 2.6048834323883057\n",
      "loss: 2.586778402328491\n",
      "loss: 2.572772979736328\n",
      "loss: 2.5761289596557617\n",
      "loss: 2.6135153770446777\n",
      "loss: 2.6900150775909424\n",
      "loss: 2.5876996517181396\n",
      "loss: 2.562394380569458\n",
      "loss: 2.756182909011841\n",
      "loss: 2.6043806076049805\n",
      "loss: 2.590602397918701\n",
      "loss: 2.5181336402893066\n",
      "loss: 2.4874954223632812\n",
      "loss: 2.549154281616211\n",
      "loss: 2.6908986568450928\n",
      "loss: 2.6816115379333496\n",
      "loss: 2.557365894317627\n",
      "loss: 2.565324306488037\n",
      "loss: 2.5712108612060547\n",
      "loss: 2.477193593978882\n",
      "loss: 2.5598535537719727\n",
      "loss: 2.6899828910827637\n",
      "loss: 2.599294662475586\n",
      "loss: 2.597553014755249\n",
      "loss: 2.5599207878112793\n",
      "loss: 2.594879150390625\n",
      "loss: 2.440465211868286\n",
      "loss: 2.5591015815734863\n",
      "loss: 2.690542697906494\n",
      "loss: 2.6389269828796387\n",
      "loss: 2.6446633338928223\n",
      "loss: 2.450465202331543\n",
      "loss: 2.5369536876678467\n",
      "loss: 2.6433162689208984\n",
      "loss: 2.6684863567352295\n",
      "loss: 2.594106912612915\n",
      "loss: 2.431413412094116\n",
      "loss: 2.655414342880249\n",
      "loss: 2.5451860427856445\n",
      "loss: 2.5472421646118164\n",
      "loss: 2.5589287281036377\n",
      "loss: 2.6477887630462646\n",
      "loss: 2.66191029548645\n",
      "loss: 2.6556365489959717\n",
      "loss: 2.5931923389434814\n",
      "loss: 2.5186359882354736\n",
      "loss: 2.6400349140167236\n",
      "loss: 2.566876173019409\n",
      "loss: 2.597296953201294\n",
      "loss: 2.5107150077819824\n",
      "loss: 2.539006471633911\n",
      "loss: 2.6162219047546387\n",
      "loss: 2.548583984375\n",
      "loss: 2.51601505279541\n",
      "loss: 2.573632001876831\n",
      "loss: 2.6136608123779297\n",
      "loss: 2.611522912979126\n",
      "loss: 2.6218819618225098\n",
      "loss: 2.5183794498443604\n",
      "loss: 2.5654146671295166\n",
      "loss: 2.495422124862671\n",
      "loss: 2.57173490524292\n",
      "loss: 2.5563125610351562\n",
      "loss: 2.6820759773254395\n",
      "loss: 2.578836441040039\n",
      "loss: 2.578958749771118\n",
      "loss: 2.507681369781494\n",
      "loss: 2.468170642852783\n",
      "loss: 2.5782623291015625\n",
      "loss: 2.6457788944244385\n",
      "loss: 2.523099899291992\n",
      "loss: 2.563239097595215\n",
      "loss: 2.5512092113494873\n",
      "loss: 2.5952517986297607\n",
      "loss: 2.527228832244873\n",
      "loss: 2.4273951053619385\n",
      "loss: 2.5884451866149902\n",
      "loss: 2.604247808456421\n",
      "loss: 2.513394355773926\n",
      "loss: 2.6326537132263184\n",
      "loss: 2.608534097671509\n",
      "loss: 2.445525884628296\n",
      "loss: 2.6876027584075928\n",
      "loss: 2.490708827972412\n",
      "loss: 2.513338327407837\n",
      "loss: 2.562319755554199\n",
      "loss: 2.5175843238830566\n",
      "loss: 2.5488781929016113\n",
      "loss: 2.4961719512939453\n",
      "loss: 2.5493006706237793\n",
      "loss: 2.5034337043762207\n",
      "loss: 2.5713326930999756\n",
      "loss: 2.5124452114105225\n",
      "loss: 2.7327823638916016\n",
      "loss: 2.4951045513153076\n",
      "loss: 2.4439852237701416\n",
      "loss: 2.5630862712860107\n",
      "loss: 2.5150530338287354\n",
      "loss: 2.5775768756866455\n",
      "loss: 2.603267192840576\n",
      "loss: 2.558681011199951\n",
      "loss: 2.569976568222046\n",
      "loss: 2.518254518508911\n",
      "loss: 2.7186055183410645\n",
      "loss: 2.5726163387298584\n",
      "loss: 2.6255099773406982\n",
      "loss: 2.4234676361083984\n",
      "loss: 2.604066848754883\n",
      "loss: 2.5240368843078613\n",
      "loss: 2.438201427459717\n",
      "loss: 2.5674164295196533\n",
      "loss: 2.6492269039154053\n",
      "loss: 2.6350114345550537\n",
      "loss: 2.4847919940948486\n",
      "loss: 2.6666104793548584\n",
      "loss: 2.5566718578338623\n",
      "loss: 2.6639440059661865\n",
      "loss: 2.5484111309051514\n",
      "loss: 2.6325156688690186\n",
      "loss: 2.5447843074798584\n",
      "loss: 2.50940203666687\n",
      "loss: 2.607985258102417\n",
      "loss: 2.5695629119873047\n",
      "loss: 2.5658371448516846\n",
      "loss: 2.5551505088806152\n",
      "loss: 2.3741366863250732\n",
      "loss: 2.5717732906341553\n",
      "loss: 2.6061675548553467\n",
      "loss: 2.6830313205718994\n",
      "loss: 2.4465081691741943\n",
      "loss: 2.638049840927124\n",
      "loss: 2.5097720623016357\n",
      "loss: 2.5533125400543213\n",
      "loss: 2.5361762046813965\n",
      "loss: 2.555509567260742\n",
      "loss: 2.4957354068756104\n",
      "loss: 2.5165786743164062\n",
      "loss: 2.5353786945343018\n",
      "loss: 2.4626214504241943\n",
      "loss: 2.467517614364624\n",
      "loss: 2.5361456871032715\n",
      "loss: 2.5203640460968018\n",
      "loss: 2.52077317237854\n",
      "loss: 2.4755303859710693\n",
      "loss: 2.5083768367767334\n",
      "loss: 2.4412009716033936\n",
      "loss: 2.5531656742095947\n",
      "loss: 2.562319755554199\n",
      "loss: 2.5827674865722656\n",
      "loss: 2.492137908935547\n",
      "loss: 2.5088210105895996\n",
      "loss: 2.4955825805664062\n",
      "loss: 2.396606683731079\n",
      "loss: 2.5362462997436523\n",
      "loss: 2.533432960510254\n",
      "loss: 2.563260793685913\n",
      "loss: 2.5799224376678467\n",
      "loss: 2.424048662185669\n",
      "loss: 2.511322259902954\n",
      "loss: 2.5939793586730957\n",
      "loss: 2.416778802871704\n",
      "loss: 2.6186671257019043\n",
      "loss: 2.5436391830444336\n",
      "loss: 2.3994393348693848\n",
      "loss: 2.5089528560638428\n",
      "loss: 2.381300687789917\n",
      "loss: 2.480856418609619\n",
      "loss: 2.5035922527313232\n",
      "loss: 2.6284947395324707\n",
      "loss: 2.4432103633880615\n",
      "loss: 2.4782464504241943\n",
      "loss: 2.567174196243286\n",
      "loss: 2.5829572677612305\n",
      "loss: 2.3567721843719482\n",
      "loss: 2.5367939472198486\n",
      "loss: 2.5252134799957275\n",
      "loss: 2.5403735637664795\n",
      "loss: 2.6437854766845703\n",
      "loss: 2.3940062522888184\n",
      "loss: 2.519827127456665\n",
      "loss: 2.6196563243865967\n",
      "loss: 2.501373052597046\n",
      "loss: 2.445788621902466\n",
      "loss: 2.6419222354888916\n",
      "loss: 2.486689329147339\n",
      "loss: 2.5284361839294434\n",
      "loss: 2.4351632595062256\n",
      "loss: 2.5309975147247314\n",
      "loss: 2.584625482559204\n",
      "loss: 2.5299887657165527\n",
      "loss: 2.5522780418395996\n",
      "loss: 2.5347869396209717\n",
      "loss: 2.3413166999816895\n",
      "loss: 2.425917625427246\n",
      "loss: 2.5200390815734863\n",
      "loss: 2.490102767944336\n",
      "loss: 2.5256917476654053\n",
      "loss: 2.591029644012451\n",
      "loss: 2.468305826187134\n",
      "loss: 2.5450966358184814\n",
      "loss: 2.4936349391937256\n",
      "loss: 2.44551157951355\n",
      "loss: 2.5257575511932373\n",
      "loss: 2.426182985305786\n",
      "loss: 2.5356898307800293\n",
      "loss: 2.460310697555542\n",
      "loss: 2.4299328327178955\n",
      "loss: 2.597306728363037\n",
      "loss: 2.5000858306884766\n",
      "loss: 2.384605646133423\n",
      "loss: 2.4960196018218994\n",
      "loss: 2.4275481700897217\n",
      "loss: 2.6248233318328857\n",
      "loss: 2.476191282272339\n",
      "loss: 2.513321876525879\n",
      "loss: 2.486077308654785\n",
      "loss: 2.4592318534851074\n",
      "loss: 2.492439031600952\n",
      "loss: 2.4546186923980713\n",
      "loss: 2.5803465843200684\n",
      "loss: 2.5657660961151123\n",
      "loss: 2.422645330429077\n",
      "loss: 2.6033124923706055\n",
      "loss: 2.40431547164917\n",
      "loss: 2.5007753372192383\n",
      "loss: 2.3976333141326904\n",
      "loss: 2.446280002593994\n",
      "loss: 2.5387158393859863\n",
      "loss: 2.5551488399505615\n",
      "loss: 2.4927196502685547\n",
      "loss: 2.4201998710632324\n",
      "loss: 2.491250991821289\n",
      "loss: 2.5548794269561768\n",
      "loss: 2.417520761489868\n",
      "loss: 2.4136548042297363\n",
      "loss: 2.4664783477783203\n",
      "loss: 2.51235032081604\n",
      "loss: 2.5246543884277344\n",
      "loss: 2.522127151489258\n",
      "loss: 2.5625336170196533\n",
      "loss: 2.5237672328948975\n",
      "loss: 2.444455862045288\n",
      "loss: 2.4730606079101562\n",
      "loss: 2.5449812412261963\n",
      "loss: 2.5382258892059326\n",
      "loss: 2.485365152359009\n",
      "loss: 2.402369976043701\n",
      "loss: 2.4453508853912354\n",
      "loss: 2.489938974380493\n",
      "loss: 2.510503053665161\n",
      "loss: 2.5905137062072754\n",
      "loss: 2.4148168563842773\n",
      "loss: 2.4501430988311768\n",
      "loss: 2.470379114151001\n",
      "loss: 2.4364147186279297\n",
      "loss: 2.613555431365967\n",
      "loss: 2.5531070232391357\n",
      "loss: 2.390744686126709\n",
      "loss: 2.439551830291748\n",
      "loss: 2.5802671909332275\n",
      "loss: 2.441819667816162\n",
      "loss: 2.5511701107025146\n",
      "loss: 2.567054510116577\n",
      "loss: 2.469728469848633\n",
      "loss: 2.6759896278381348\n",
      "loss: 2.5593650341033936\n",
      "loss: 2.484645366668701\n",
      "loss: 2.528470993041992\n",
      "loss: 2.438115358352661\n",
      "loss: 2.4914376735687256\n",
      "loss: 2.432295560836792\n",
      "loss: 2.4573233127593994\n",
      "loss: 2.4890849590301514\n",
      "loss: 2.504464626312256\n",
      "loss: 2.3940141201019287\n",
      "loss: 2.541395902633667\n",
      "loss: 2.5590364933013916\n",
      "loss: 2.5895137786865234\n",
      "loss: 2.358832836151123\n",
      "loss: 2.5965535640716553\n",
      "loss: 2.6209073066711426\n",
      "loss: 2.6354751586914062\n",
      "loss: 2.460391044616699\n",
      "loss: 2.449270725250244\n",
      "loss: 2.5133867263793945\n",
      "loss: 2.4629034996032715\n",
      "loss: 2.475292921066284\n",
      "loss: 2.5054774284362793\n",
      "loss: 2.5903120040893555\n",
      "loss: 2.431274652481079\n",
      "loss: 2.440139055252075\n",
      "loss: 2.503638505935669\n",
      "loss: 2.498767852783203\n",
      "loss: 2.551522970199585\n",
      "loss: 2.4626481533050537\n",
      "loss: 2.3995211124420166\n",
      "loss: 2.634073495864868\n",
      "loss: 2.470949172973633\n",
      "loss: 2.5852904319763184\n",
      "loss: 2.462820291519165\n",
      "loss: 2.5484509468078613\n",
      "loss: 2.509120225906372\n",
      "loss: 2.548586845397949\n",
      "loss: 2.511718511581421\n",
      "loss: 2.470841884613037\n",
      "loss: 2.560236930847168\n",
      "loss: 2.5452053546905518\n",
      "loss: 2.6943886280059814\n",
      "loss: 2.4785101413726807\n",
      "loss: 2.564800977706909\n",
      "loss: 2.4577529430389404\n",
      "loss: 2.380690336227417\n",
      "loss: 2.490567445755005\n",
      "loss: 2.4981939792633057\n",
      "loss: 2.4805145263671875\n",
      "loss: 2.4785990715026855\n",
      "loss: 2.471336603164673\n",
      "loss: 2.532954454421997\n",
      "loss: 2.4330856800079346\n",
      "loss: 2.4059107303619385\n",
      "loss: 2.564570426940918\n",
      "loss: 2.506786823272705\n",
      "loss: 2.5609493255615234\n",
      "loss: 2.556171178817749\n",
      "loss: 2.4416420459747314\n",
      "loss: 2.49434232711792\n",
      "loss: 2.5873916149139404\n",
      "loss: 2.573699712753296\n",
      "loss: 2.391230583190918\n",
      "loss: 2.557318925857544\n",
      "loss: 2.5509791374206543\n",
      "loss: 2.4631781578063965\n",
      "loss: 2.460728645324707\n",
      "loss: 2.477263927459717\n",
      "loss: 2.596902847290039\n",
      "loss: 2.5110909938812256\n",
      "loss: 2.658886671066284\n",
      "loss: 2.444880247116089\n",
      "loss: 2.5295894145965576\n",
      "loss: 2.508211374282837\n",
      "loss: 2.4522063732147217\n",
      "loss: 2.3991479873657227\n",
      "loss: 2.536637306213379\n",
      "loss: 2.585805892944336\n",
      "loss: 2.427577495574951\n",
      "loss: 2.527397871017456\n",
      "loss: 2.4854443073272705\n",
      "loss: 2.5337026119232178\n",
      "loss: 2.521150588989258\n",
      "loss: 2.460855722427368\n",
      "loss: 2.422849416732788\n",
      "loss: 2.370823860168457\n",
      "loss: 2.434081554412842\n",
      "loss: 2.4951772689819336\n",
      "loss: 2.4627904891967773\n",
      "loss: 2.5905885696411133\n",
      "loss: 2.5920989513397217\n",
      "loss: 2.41814923286438\n",
      "loss: 2.421481132507324\n",
      "loss: 2.4548919200897217\n",
      "loss: 2.5235021114349365\n",
      "loss: 2.5185585021972656\n",
      "loss: 2.4163975715637207\n",
      "loss: 2.480026960372925\n",
      "loss: 2.4855470657348633\n",
      "loss: 2.556457757949829\n",
      "loss: 2.3889787197113037\n",
      "loss: 2.396425485610962\n",
      "loss: 2.434828996658325\n",
      "loss: 2.521761417388916\n",
      "loss: 2.625199317932129\n",
      "loss: 2.380669593811035\n",
      "loss: 2.5142223834991455\n",
      "loss: 2.538891553878784\n",
      "loss: 2.564408540725708\n",
      "loss: 2.5294599533081055\n",
      "loss: 2.4825615882873535\n",
      "loss: 2.522390842437744\n",
      "loss: 2.4579195976257324\n",
      "loss: 2.413403272628784\n",
      "loss: 2.5577385425567627\n",
      "loss: 2.538673162460327\n",
      "loss: 2.3816678524017334\n",
      "loss: 2.4867470264434814\n",
      "loss: 2.4423880577087402\n",
      "loss: 2.5974488258361816\n",
      "loss: 2.4790244102478027\n",
      "loss: 2.4169116020202637\n",
      "loss: 2.405393362045288\n",
      "loss: 2.4719455242156982\n",
      "loss: 2.377139091491699\n",
      "loss: 2.4462172985076904\n",
      "loss: 2.4939517974853516\n",
      "loss: 2.4493775367736816\n",
      "loss: 2.545633554458618\n",
      "loss: 2.468322277069092\n",
      "loss: 2.4538252353668213\n",
      "loss: 2.589962959289551\n",
      "loss: 2.408653497695923\n",
      "loss: 2.500859498977661\n",
      "loss: 2.406411647796631\n",
      "loss: 2.549142837524414\n",
      "loss: 2.4595730304718018\n",
      "loss: 2.4359593391418457\n",
      "loss: 2.465571403503418\n",
      "loss: 2.498755931854248\n",
      "loss: 2.6302132606506348\n",
      "loss: 2.4146697521209717\n",
      "loss: 2.5104947090148926\n",
      "loss: 2.3615119457244873\n",
      "loss: 2.4722416400909424\n",
      "loss: 2.4928369522094727\n",
      "loss: 2.468118667602539\n",
      "loss: 2.510175943374634\n",
      "loss: 2.532785654067993\n",
      "loss: 2.456855058670044\n",
      "loss: 2.403700351715088\n",
      "loss: 2.5919008255004883\n",
      "loss: 2.4718120098114014\n",
      "loss: 2.4895427227020264\n",
      "loss: 2.4070987701416016\n",
      "loss: 2.4331536293029785\n",
      "loss: 2.424010753631592\n",
      "loss: 2.4299521446228027\n",
      "loss: 2.3641293048858643\n",
      "loss: 2.350613832473755\n",
      "loss: 2.561361789703369\n",
      "loss: 2.392202854156494\n",
      "loss: 2.511427640914917\n",
      "loss: 2.3697357177734375\n",
      "loss: 2.4168760776519775\n",
      "loss: 2.46537709236145\n",
      "loss: 2.4176535606384277\n",
      "loss: 2.59269380569458\n",
      "loss: 2.4142966270446777\n",
      "loss: 2.386369466781616\n",
      "loss: 2.462500810623169\n",
      "loss: 2.4462828636169434\n",
      "loss: 2.5262153148651123\n",
      "loss: 2.3799474239349365\n",
      "loss: 2.6373417377471924\n",
      "loss: 2.38862681388855\n",
      "loss: 2.4705593585968018\n",
      "loss: 2.4583663940429688\n",
      "loss: 2.5732219219207764\n",
      "loss: 2.5721163749694824\n",
      "loss: 2.4805126190185547\n",
      "loss: 2.4723198413848877\n",
      "loss: 2.582810163497925\n",
      "loss: 2.491410970687866\n",
      "loss: 2.4018256664276123\n",
      "loss: 2.391705274581909\n",
      "loss: 2.4202492237091064\n",
      "loss: 2.5745162963867188\n",
      "loss: 2.4124176502227783\n",
      "loss: 2.6116344928741455\n",
      "loss: 2.5785489082336426\n",
      "loss: 2.3848953247070312\n",
      "loss: 2.619035482406616\n",
      "loss: 2.4334609508514404\n",
      "loss: 2.534632682800293\n",
      "loss: 2.384725332260132\n",
      "loss: 2.471384048461914\n",
      "loss: 2.5246152877807617\n",
      "loss: 2.4160633087158203\n",
      "loss: 2.4407718181610107\n",
      "loss: 2.4901649951934814\n",
      "loss: 2.48317289352417\n",
      "loss: 2.490490198135376\n",
      "loss: 2.510528326034546\n",
      "loss: 2.5282301902770996\n",
      "loss: 2.4663567543029785\n",
      "loss: 2.4887256622314453\n",
      "loss: 2.417559862136841\n",
      "loss: 2.4930474758148193\n",
      "loss: 2.457150936126709\n",
      "loss: 2.4706246852874756\n",
      "loss: 2.6090524196624756\n",
      "loss: 2.5888872146606445\n",
      "loss: 2.3176398277282715\n",
      "loss: 2.391038656234741\n",
      "loss: 2.5072832107543945\n",
      "loss: 2.4869301319122314\n",
      "loss: 2.3837778568267822\n",
      "loss: 2.3939623832702637\n",
      "loss: 2.458224058151245\n",
      "loss: 2.384221315383911\n",
      "loss: 2.5005340576171875\n",
      "loss: 2.5516836643218994\n",
      "loss: 2.4829673767089844\n",
      "loss: 2.4979944229125977\n",
      "loss: 2.450772762298584\n",
      "loss: 2.6313087940216064\n",
      "loss: 2.578366756439209\n",
      "loss: 2.536289930343628\n",
      "loss: 2.424860954284668\n",
      "loss: 2.564246892929077\n",
      "loss: 2.4728500843048096\n",
      "loss: 2.5030405521392822\n",
      "loss: 2.4473085403442383\n",
      "loss: 2.422910451889038\n",
      "loss: 2.4644975662231445\n",
      "loss: 2.5190322399139404\n",
      "loss: 2.5459983348846436\n",
      "loss: 2.538426399230957\n",
      "loss: 2.4926366806030273\n",
      "loss: 2.5411343574523926\n",
      "loss: 2.4490222930908203\n",
      "loss: 2.460486650466919\n",
      "loss: 2.5724265575408936\n",
      "loss: 2.3662891387939453\n",
      "loss: 2.520871877670288\n",
      "loss: 2.627652406692505\n",
      "loss: 2.6015536785125732\n",
      "loss: 2.561849594116211\n",
      "loss: 2.572082996368408\n",
      "loss: 2.445150136947632\n",
      "loss: 2.4830143451690674\n",
      "loss: 2.4427247047424316\n",
      "loss: 2.4879069328308105\n",
      "loss: 2.3717429637908936\n",
      "loss: 2.5301990509033203\n",
      "loss: 2.5234382152557373\n",
      "loss: 2.5762276649475098\n",
      "loss: 2.3588380813598633\n",
      "loss: 2.429462194442749\n",
      "loss: 2.4191906452178955\n",
      "loss: 2.50319242477417\n",
      "loss: 2.5264697074890137\n",
      "loss: 2.3453595638275146\n",
      "loss: 2.491833448410034\n",
      "loss: 2.4451348781585693\n",
      "loss: 2.395092725753784\n",
      "loss: 2.610133409500122\n",
      "loss: 2.381601572036743\n",
      "loss: 2.5247600078582764\n",
      "loss: 2.4536006450653076\n",
      "loss: 2.436025619506836\n",
      "loss: 2.4183311462402344\n",
      "loss: 2.3144829273223877\n",
      "loss: 2.471135377883911\n",
      "loss: 2.572934627532959\n",
      "loss: 2.465390920639038\n",
      "loss: 2.59637188911438\n",
      "loss: 2.495410203933716\n",
      "loss: 2.3347361087799072\n",
      "loss: 2.469698905944824\n",
      "loss: 2.4357762336730957\n",
      "loss: 2.463754415512085\n",
      "loss: 2.4943246841430664\n",
      "loss: 2.421546459197998\n",
      "loss: 2.4148921966552734\n",
      "loss: 2.513732433319092\n",
      "loss: 2.366122245788574\n",
      "loss: 2.4700570106506348\n",
      "loss: 2.593759059906006\n",
      "loss: 2.4306070804595947\n",
      "loss: 2.4935550689697266\n",
      "loss: 2.4816689491271973\n",
      "loss: 2.3836264610290527\n",
      "loss: 2.30765962600708\n",
      "loss: 2.5189051628112793\n",
      "loss: 2.366910457611084\n",
      "loss: 2.375774621963501\n",
      "loss: 2.443359136581421\n",
      "loss: 2.4722917079925537\n",
      "loss: 2.523076057434082\n",
      "loss: 2.44582200050354\n",
      "loss: 2.6225385665893555\n",
      "loss: 2.4350106716156006\n",
      "loss: 2.4656805992126465\n",
      "loss: 2.4447150230407715\n",
      "loss: 2.4465253353118896\n",
      "loss: 2.3909504413604736\n",
      "loss: 2.554527759552002\n",
      "loss: 2.5822269916534424\n",
      "loss: 2.4526474475860596\n",
      "loss: 2.4588375091552734\n",
      "loss: 2.5323431491851807\n",
      "loss: 2.3476850986480713\n",
      "loss: 2.600881576538086\n",
      "loss: 2.5271899700164795\n",
      "loss: 2.484226703643799\n",
      "loss: 2.5732011795043945\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 10 == 0:\n",
    "        print(f'loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "llo br. ave aviasurf my, mayo t ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;\n",
      "\n",
      "Whe, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty dedo bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "By bre ndy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n sar; my w, fredeeyong\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, pannd e Yolde Th likl beamen, tofr,\n",
      "n s Byo tred ceathe, il ivilde w\n",
      "O ff y\n",
      "Fivede? ig aiMy, I ivis muofounce herevern outh f athawendesees yof th withind be wameats tsteer y blitow,\n",
      "Ye m o ditoshyd me, ch rte u hart ararwsa\n",
      "Wou fe,\n",
      "INurathoune\n",
      "IESSARin,\n",
      "MIOLened sust tl.\n",
      "S:\n",
      "NMy BAnind g.\n",
      "iudshank\n",
      "An chin is a arokisupxaseru t w ity merwo al LOLo bebte loolld worinero ya l aknge ond thal ttry b's mo ge ck.\n",
      "\n",
      "gh, inketilllin trewnutud t ar,\n",
      "WAnt cithapis Zimponcrdistherdrtes saure ' erpoperrposthelind y ss of hef th\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "gen_seq = m.generate(idx, max_new_tokens=1000)\n",
    "print(decode(gen_seq[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Simple version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.randn((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1]  # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x  # (B, T, C) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow, xbow2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Softmax version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16]) torch.Size([4, 16, 8]) torch.Size([4, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)    \n",
    "wei = q @ k.transpose(-2, -1)\n",
    "print(q.shape, k.transpose(-2, -1).shape, wei.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)    \n",
    "wei = q @ k.transpose(-2, -1) * head_size ** 0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "v = value(x)  # (B, T, head_size)\n",
    "out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
